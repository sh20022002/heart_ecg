{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  #  for progress bar\n",
    "import collections\n",
    "import random\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.csv_file_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith('.csv')]\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        #  Label mapping for classification\n",
    "        label_mapping = {'N': 0, 'L': 1, 'R': 2, 'A': 3, 'V': 4}\n",
    "\n",
    "        for file_path in self.csv_file_paths:\n",
    "            file_name = os.path.basename(file_path).replace('.csv', '')  # Get ECG file name (e.g., \"100\")\n",
    "            annotation_path = os.path.join(directory, f\"{file_name}annotations.txt\")  # Matching annotation file\n",
    "\n",
    "            #  Load ECG Data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = df.columns.str.replace(\"'\", \"\").str.strip()  # Fix column names\n",
    "            feature_columns = df.columns[1:3]  # Select MLII & V5\n",
    "            ecg_samples = df[feature_columns].values\n",
    "\n",
    "            #  Load Annotations (Labels) with error handling\n",
    "            try:\n",
    "                with open(annotation_path, \"r\") as file:\n",
    "                    lines = file.readlines()[1:]  #  Skip header row\n",
    "\n",
    "                cleaned_data = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()  #  Split by whitespace\n",
    "                    if len(parts) >= 3:  #  Ensure at least 3 columns exist\n",
    "                        sample_idx, label = parts[1], parts[2]\n",
    "                        label = \"\".join(filter(str.isalpha, label))  #  Extract only valid label letters\n",
    "                        cleaned_data.append([int(sample_idx), label])\n",
    "\n",
    "                #  Convert to DataFrame\n",
    "                annotations = pd.DataFrame(cleaned_data, columns=[\"Sample\", \"Type\"])\n",
    "\n",
    "                #  Filter valid labels\n",
    "                annotations = annotations[annotations[\"Type\"].isin(label_mapping.keys())]\n",
    "                annotations[\"Label\"] = annotations[\"Type\"].map(label_mapping)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ğŸš¨ Skipping {annotation_path} due to error: {e}\")\n",
    "                continue  #  Skip this file instead of crashing\n",
    "            \n",
    "            #  Match Labels to ECG Samples\n",
    "            labels = np.zeros(len(df), dtype=np.int64)  # Default all labels to 'N' (0)\n",
    "            for _, row in annotations.iterrows():\n",
    "                sample_idx = int(row['Sample'])  # Ensure it's an integer index\n",
    "                if 0 <= sample_idx < len(labels):  # Ensure index is valid\n",
    "                    labels[sample_idx] = row['Label']  # Assign label\n",
    "\n",
    "            self.data.append(ecg_samples)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        self.labels = np.concatenate(self.labels, axis=0)\n",
    "\n",
    "        #  Print label distribution before balancing\n",
    "        label_counts = collections.Counter(self.labels.tolist())\n",
    "        print(\"ğŸ“Œ Before Balancing - Label Counts:\", label_counts)\n",
    "\n",
    "        #  Balance dataset by undersampling class '0'\n",
    "        zero_class_indices = [i for i, label in enumerate(self.labels) if label == 0]\n",
    "        non_zero_indices = [i for i, label in enumerate(self.labels) if label != 0]\n",
    "\n",
    "        #  Keep a maximum of 2x the non-zero labels for class '0'\n",
    "        random.shuffle(zero_class_indices)\n",
    "        zero_class_sample_size = min(len(zero_class_indices), len(non_zero_indices) * 2)\n",
    "        zero_class_indices = zero_class_indices[:zero_class_sample_size]\n",
    "\n",
    "        #  Combine balanced dataset\n",
    "        selected_indices = zero_class_indices + non_zero_indices\n",
    "        random.shuffle(selected_indices)\n",
    "\n",
    "        #  Update dataset\n",
    "        self.data = self.data[selected_indices]\n",
    "        self.labels = self.labels[selected_indices]\n",
    "\n",
    "        #  Print label distribution after balancing\n",
    "        label_counts = Counter(self.labels.tolist())\n",
    "        print(\"ğŸ“Œ After Balancing - Label Counts:\", label_counts)\n",
    "\n",
    "        #  **Normalize Data**\n",
    "        print(\"ğŸ“Œ Applying StandardScaler normalization...\")\n",
    "        scaler = StandardScaler()\n",
    "        self.data = scaler.fit_transform(self.data)  # Normalize features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        sample = sample.unsqueeze(0)  # âœ… Ensure shape (seq_length, input_size)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3, output_size=5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #  Add dropout layer to prevent overfitting\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_size, self.num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        #  Add batch normalization for stable training\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Ensure input is 3D (batch_size, seq_length, input_size)\n",
    "        if x.dim() == 2:  \n",
    "            x = x.unsqueeze(1)  # Add sequence_length = 1 if missing\n",
    "\n",
    "        #  Ensure hidden state batch size matches input batch size\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        #  Forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Pass hidden states\n",
    "        out = self.batch_norm(out[:, -1, :])  # Apply batch normalization\n",
    "        out = self.fc(out)  # Fully connected layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the LSTM Model for ECG Data\n",
    "\n",
    "The `LSTMModel` class defines a Long Short-Term Memory (LSTM) neural network designed for processing ECG data. LSTMs are particularly effective for sequential data, making them suitable for analyzing heartbeat patterns over time.\n",
    "\n",
    "### Explanation of the Components\n",
    "\n",
    "- **`input_size`**: Defines the number of input features per time step (e.g., number of ECG leads used).\n",
    "- **`hidden_size`**: Specifies the number of hidden units in each LSTM layer, controlling model complexity.\n",
    "- **`num_layers`**: Determines how many stacked LSTM layers are used for learning complex patterns.\n",
    "- **`output_size`**: Sets the dimension of the final output (e.g., number of classification categories).\n",
    "- **`lstm`**: The core sequential processing unit, which learns dependencies over time.\n",
    "- **`fc`**: A fully connected layer that maps the LSTM's last output to the desired prediction.\n",
    "\n",
    "### Explanation of the Forward Pass\n",
    "\n",
    "- Initializes hidden and cell states (`h0`, `c0`) as zero tensors, ensuring proper state tracking.\n",
    "- Feeds the input sequence `x` through the LSTM layers.\n",
    "- Extracts only the last time stepâ€™s output (`out[:, -1, :]`), as it holds the learned representation for classification.\n",
    "- Applies the fully connected layer (`fc`) to generate final predictions.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- LSTMs are well-suited for time-series and sequential analysis, making them effective for ECG classification.\n",
    "- The use of multiple LSTM layers helps capture long-term dependencies in heartbeat sequences.\n",
    "- Selecting only the last time stepâ€™s output ensures efficient prediction while reducing computational overhead.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Adjusting `hidden_size` and `num_layers` can significantly impact model performance.\n",
    "- Ensure input sequences have the correct shape (`batch_size, time_steps, input_size`).\n",
    "- Using GPU acceleration (`.to(device)`) improves training speed and efficiency.\n",
    "\n",
    "This LSTM model serves as a powerful tool for ECG analysis, capable of learning complex sequential dependencies and making accurate predictions based on time-series heartbeat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, return_logits=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataloader.\n",
    "\n",
    "    Works for both mid-training and post-training evaluation.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for evaluation.\n",
    "        device (torch.device): The device to run evaluation on.\n",
    "        return_logits (bool): If True, returns logits along with loss and accuracy.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_loss, accuracy) or (avg_loss, accuracy, logits)\n",
    "    \"\"\"\n",
    "    was_training = model.training  # Store if model was in training mode\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_logits = [] if return_logits else None  # Collect logits if needed\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for validation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "            \n",
    "            outputs = model(inputs)  #  Fixed variable name (`inputs` instead of `sampels`)\n",
    "            loss = F.cross_entropy(outputs, targets)  #  Compute loss correctly\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)  # Get predictions\n",
    "            correct += (predicted == targets).sum().item()  #  Correct accuracy calculation\n",
    "            total += targets.size(0)\n",
    "\n",
    "            if return_logits:\n",
    "                all_logits.append(outputs.cpu())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader))  #  Avoid division by zero\n",
    "    accuracy = correct / max(1, total)  #  Avoid division by zero\n",
    "\n",
    "    model.train(was_training)  # Restore original training state\n",
    "\n",
    "    if return_logits:\n",
    "        return avg_loss, accuracy, torch.cat(all_logits, dim=0)  #  Return logits if needed\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Model Evaluation Process\n",
    "\n",
    "Evaluating the model is essential for assessing its performance during and after training. The `evaluate` function helps compute loss and accuracy on a validation or test dataset to monitor model progress.\n",
    "\n",
    "### How the Evaluation Function Works\n",
    "\n",
    "This function takes in a trained model, a dataset loader, and a device specification (`CPU` or `GPU`) to evaluate model performance.\n",
    "\n",
    "### key Considerations\n",
    "\n",
    "- Ensure the evaluation dataset is separate from the training data.\n",
    "- Use a consistent batch size during evaluation for stable results.\n",
    "- If using a multi-class classification problem, adapt `F.cross_entropy` accordingly.\n",
    "\n",
    "This evaluation function provides an efficient way to assess model performance, ensuring continuous monitoring and improvement throughout training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(epoch, model, batch_idx, optimizer, loss, accuracy, path=\"model\"):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint only if:\n",
    "    1. No previous model exists.\n",
    "    2. New model has a higher accuracy than the previous one.\n",
    "    \n",
    "    Deletes inferior models automatically.\n",
    "    \"\"\"\n",
    "    checkpoint_dir = os.path.dirname(path)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if checkpoint_dir and not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        print(f\"ğŸ“ Created directory: {checkpoint_dir}\")\n",
    "\n",
    "    # Check if a previous checkpoint exists\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            prev_checkpoint = torch.load(path, map_location=\"cpu\")  # Load safely\n",
    "            prev_accuracy = prev_checkpoint.get('accuracy', 0)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading existing checkpoint: {e}\")\n",
    "            prev_accuracy = 0  # Assume no valid previous accuracy\n",
    "\n",
    "        # Keep only if accuracy is better\n",
    "        if accuracy <= prev_accuracy:\n",
    "            print(f\"ğŸš« New model NOT saved. Accuracy {accuracy:.4f} â‰¤ {prev_accuracy:.4f}\")\n",
    "            return  # Skip saving\n",
    "\n",
    "        # Delete previous checkpoint (ensuring it's removed)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "            print(f\"ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: {prev_accuracy:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to delete old checkpoint: {e}\")\n",
    "\n",
    "    #  Save the best model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'batch_idx': batch_idx,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }, path)\n",
    "\n",
    "    print(f\" New Best Checkpoint Saved at {path} (Accuracy: {accuracy:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Checkpointing\n",
    "\n",
    "Saving and loading model checkpoints is crucial for maintaining training progress, preventing data loss, and resuming interrupted training efficiently. The `save_checkpoint` and `load_checkpoint` functions manage this process by storing and retrieving model parameters.\n",
    "\n",
    "### Saving Model Checkpoints\n",
    "\n",
    "This function saves the model state along with training metadata, such as the current epoch, batch index, optimizer state, and performance metrics.\n",
    "\n",
    "\n",
    "\n",
    "### Loading Model Checkpoints\n",
    "\n",
    "This function restores a saved model checkpoint, enabling training continuation without losing progress.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Prevents loss of progress due to system crashes or interruptions.\n",
    "- Allows for training continuation across different sessions or machines.\n",
    "- Enables model fine-tuning by starting from a pre-trained checkpoint.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Ensure the correct model and optimizer classes are passed when loading.\n",
    "- Store checkpoints periodically to avoid losing significant progress.\n",
    "- If resuming on a different machine, verify that the saved checkpoint is compatible with the new environment.\n",
    "\n",
    "By implementing checkpointing, deep learning workflows become more efficient, minimizing training redundancy and ensuring reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model_class, optimizer_class, device, checkpoint_dir=\"model/\", model_args=None, optimizer_args=None):\n",
    "    \n",
    "    checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pth\")))\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        print(\"ğŸ†• No checkpoint found â€” starting from scratch.\")\n",
    "        return None, None, 0, 0, None, None\n",
    "\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "\n",
    "    model_args = model_args or {}\n",
    "    optimizer_args = optimizer_args or {\"lr\": 0.001}\n",
    "\n",
    "    model = model_class(**model_args).to(device)\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_args)\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # Will raise if mismatched\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Checkpoint incompatible: {e}\")\n",
    "        print(\"ğŸ†• Skipping loading, starting fresh model.\")\n",
    "        return None, None, 0, 0, None, None\n",
    "\n",
    "    start_epoch = checkpoint.get('epoch', 0)\n",
    "    start_batch_idx = checkpoint.get('batch_idx', 0)\n",
    "    loss = checkpoint.get('loss', None)\n",
    "    accuracy = checkpoint.get('accuracy', None)\n",
    "\n",
    "    print(f\"âœ… Loaded checkpoint from {latest_checkpoint} (Epoch {start_epoch})\")\n",
    "    return model, optimizer, start_epoch, start_batch_idx, loss, accuracy\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, criterion, device, total_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the model, resuming from the last checkpoint if available.\n",
    "    \n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): Training data loader.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Training device (CPU/GPU).\n",
    "        total_epochs (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    model_args = {\"input_size\": 2, \"hidden_size\": 128, \"num_layers\": 2, \"dropout\":0.3, \"output_size\": 5}\n",
    "    \n",
    "\n",
    "    optimizer_args = {\"lr\": 0.001}\n",
    "\n",
    "    #  Load latest checkpoint or start fresh\n",
    "\n",
    "    model, optimizer, start_epoch, start_batch_idx, loss, accuracy = load_checkpoint(\n",
    "        model_class=LSTMModel, \n",
    "        optimizer_class=torch.optim.Adam, \n",
    "        device=device,\n",
    "        checkpoint_dir=\"model/\",\n",
    "        model_args=model_args, \n",
    "        optimizer_args=optimizer_args\n",
    "    )\n",
    "   \n",
    "    #  Handle missing checkpoint (Initialize a new model if needed)\n",
    "    if model is None:\n",
    "        model = LSTMModel(**model_args).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        start_epoch, start_batch_idx = 0, 0\n",
    "        print(\"âš ï¸ No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "    model.train()  #  Ensure model is in training mode\n",
    "    criterion.to(device)  # Move loss function to the correct device\n",
    "\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        print(f\"\\nğŸš€ Started Epoch {epoch+1}/{total_epochs}\")\n",
    "        for batch_idx, (samples, labels) in enumerate(train_loader):\n",
    "            #  Skip already processed batches in the current epoch\n",
    "            if epoch == start_epoch and batch_idx < start_batch_idx:\n",
    "                continue  \n",
    "\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(samples)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # eval\n",
    "        avg_loss, accuracy = evaluate(model, train_loader, device, return_logits=False)\n",
    "        \n",
    "        #  Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(epoch, model, batch_idx, optimizer, loss, accuracy, path=f\"model/checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epochs}], Batch {batch_idx}/{len(train_loader)} - Val Loss = {avg_loss:.4f}, Val Accuracy = {accuracy:.4%}\")\n",
    "\n",
    "    print(\"\\nğŸ‰ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Model Training Process\n",
    "\n",
    "Training a deep learning model involves iterating through the dataset, optimizing weights, and monitoring performance over multiple epochs. The `train` function facilitates this process while incorporating checkpointing for training resumption.\n",
    "\n",
    "### How the Training Function Works\n",
    "\n",
    "This function takes a dataset loader, a loss function, and a device specification to train the model. It also supports saving and resuming training from a checkpoint.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Allows efficient training with automatic resumption.\n",
    "- Provides periodic evaluation to track learning progress.\n",
    "- Enables early stopping or hyperparameter tuning based on validation results.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Ensure batch skipping logic works correctly when resuming training.\n",
    "- Adjust learning rates dynamically if performance stagnates.\n",
    "- Regularly monitor validation accuracy to detect overfitting.\n",
    "\n",
    "This training function ensures a robust workflow, combining efficient model training with checkpointing and evaluation to optimize ECG data analysis models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Before Balancing - Label Counts: Counter({0: 31174990, 1: 8075, 2: 7259, 4: 7130, 3: 2546})\n",
      "ğŸ“Œ After Balancing - Label Counts: Counter({0: 50020, 1: 8075, 2: 7259, 4: 7130, 3: 2546})\n",
      "ğŸ“Œ Applying StandardScaler normalization...\n",
      "Dataset size: 75030\n"
     ]
    }
   ],
   "source": [
    "# Assuming paths and data are correct\n",
    "data_path = \"../mitbih_database\"\n",
    "dataset = ECGDataset(data_path)\n",
    "\n",
    "print(f'Dataset size: {dataset.__len__()}')\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on : cpu\n",
      "âš ï¸ Checkpoint incompatible: Error(s) in loading state_dict for LSTMModel:\n",
      "\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\". \n",
      "ğŸ†• Skipping loading, starting fresh model.\n",
      "âš ï¸ No checkpoint found, starting training from scratch.\n",
      "\n",
      "ğŸš€ Started Epoch 1/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8023 â‰¤ 0.8399\n",
      "Epoch [1/20], Batch 1875/1876 - Val Loss = 0.5236, Val Accuracy = 80.2346%\n",
      "\n",
      "ğŸš€ Started Epoch 2/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8282 â‰¤ 0.8558\n",
      "Epoch [2/20], Batch 1875/1876 - Val Loss = 0.4682, Val Accuracy = 82.8152%\n",
      "\n",
      "ğŸš€ Started Epoch 3/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8291 â‰¤ 0.8530\n",
      "Epoch [3/20], Batch 1875/1876 - Val Loss = 0.4461, Val Accuracy = 82.9068%\n",
      "\n",
      "ğŸš€ Started Epoch 4/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8453 â‰¤ 0.8559\n",
      "Epoch [4/20], Batch 1875/1876 - Val Loss = 0.4278, Val Accuracy = 84.5279%\n",
      "\n",
      "ğŸš€ Started Epoch 5/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8520 â‰¤ 0.8551\n",
      "Epoch [5/20], Batch 1875/1876 - Val Loss = 0.4161, Val Accuracy = 85.1976%\n",
      "\n",
      "ğŸš€ Started Epoch 6/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8500)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_5.pth (Accuracy: 0.8510)\n",
      "Epoch [6/20], Batch 1875/1876 - Val Loss = 0.4137, Val Accuracy = 85.1043%\n",
      "\n",
      "ğŸš€ Started Epoch 7/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8536 â‰¤ 0.8591\n",
      "Epoch [7/20], Batch 1875/1876 - Val Loss = 0.4053, Val Accuracy = 85.3575%\n",
      "\n",
      "ğŸš€ Started Epoch 8/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8542 â‰¤ 0.8549\n",
      "Epoch [8/20], Batch 1875/1876 - Val Loss = 0.3977, Val Accuracy = 85.4192%\n",
      "\n",
      "ğŸš€ Started Epoch 9/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8561 â‰¤ 0.8602\n",
      "Epoch [9/20], Batch 1875/1876 - Val Loss = 0.3974, Val Accuracy = 85.6091%\n",
      "\n",
      "ğŸš€ Started Epoch 10/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8576)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_9.pth (Accuracy: 0.8577)\n",
      "Epoch [10/20], Batch 1875/1876 - Val Loss = 0.3899, Val Accuracy = 85.7707%\n",
      "\n",
      "ğŸš€ Started Epoch 11/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8585 â‰¤ 0.8594\n",
      "Epoch [11/20], Batch 1875/1876 - Val Loss = 0.3914, Val Accuracy = 85.8457%\n",
      "\n",
      "ğŸš€ Started Epoch 12/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8583)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_11.pth (Accuracy: 0.8600)\n",
      "Epoch [12/20], Batch 1875/1876 - Val Loss = 0.3865, Val Accuracy = 86.0039%\n",
      "\n",
      "ğŸš€ Started Epoch 13/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8576 â‰¤ 0.8577\n",
      "Epoch [13/20], Batch 1875/1876 - Val Loss = 0.3871, Val Accuracy = 85.7590%\n",
      "\n",
      "ğŸš€ Started Epoch 14/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8586 â‰¤ 0.8595\n",
      "Epoch [14/20], Batch 1875/1876 - Val Loss = 0.3862, Val Accuracy = 85.8640%\n",
      "\n",
      "ğŸš€ Started Epoch 15/20\n",
      "ğŸš« New model NOT saved. Accuracy 0.8589 â‰¤ 0.8622\n",
      "Epoch [15/20], Batch 1875/1876 - Val Loss = 0.3847, Val Accuracy = 85.8873%\n",
      "\n",
      "ğŸš€ Started Epoch 16/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8609)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_15.pth (Accuracy: 0.8613)\n",
      "Epoch [16/20], Batch 1875/1876 - Val Loss = 0.3809, Val Accuracy = 86.1339%\n",
      "\n",
      "ğŸš€ Started Epoch 17/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8595)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_16.pth (Accuracy: 0.8608)\n",
      "Epoch [17/20], Batch 1875/1876 - Val Loss = 0.3800, Val Accuracy = 86.0756%\n",
      "\n",
      "ğŸš€ Started Epoch 18/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8590)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_17.pth (Accuracy: 0.8610)\n",
      "Epoch [18/20], Batch 1875/1876 - Val Loss = 0.3785, Val Accuracy = 86.1039%\n",
      "\n",
      "ğŸš€ Started Epoch 19/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8606)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_18.pth (Accuracy: 0.8627)\n",
      "Epoch [19/20], Batch 1875/1876 - Val Loss = 0.3781, Val Accuracy = 86.2738%\n",
      "\n",
      "ğŸš€ Started Epoch 20/20\n",
      "ğŸ—‘ï¸ Deleted inferior checkpoint (Accuracy: 0.8589)\n",
      " New Best Checkpoint Saved at model/checkpoint_epoch_19.pth (Accuracy: 0.8616)\n",
      "Epoch [20/20], Batch 1875/1876 - Val Loss = 0.3781, Val Accuracy = 86.1572%\n",
      "\n",
      "ğŸ‰ Training complete!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Runing on : cuda') if torch.cuda.is_available() else print('Runing on : cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "train(train_loader, criterion, device, total_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data Loading for ECG Training\n",
    "\n",
    "Properly loading and splitting a dataset is crucial for effective model training and evaluation. The following approach ensures that the ECG dataset is structured for training while maintaining a separate test set for validation.\n",
    "\n",
    "### How Data is Loaded and Split\n",
    "\n",
    "- Loads the ECG dataset from the specified directory.\n",
    "- Ensures all `.csv` files in the directory are processed correctly.\n",
    "\n",
    "* Splits the dataset into an **80% training set** and a **20% test set**.\n",
    "* Uses `torch.utils.data.random_split()` to randomly partition the data.\n",
    "\n",
    "- Creates `DataLoader` objects for both the training and test datasets.\n",
    "- Enables **batch processing** with a batch size of 32.\n",
    "- `shuffle=True` is set for the training data to prevent model overfitting to specific data patterns.\n",
    "- The test dataset is not shuffled to maintain consistency in evaluation.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Ensures a proper training-test split for model evaluation.\n",
    "- Facilitates efficient data loading using PyTorch's `DataLoader`.\n",
    "- Randomization in the training set prevents the model from memorizing specific sequences.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- The dataset split ratio can be adjusted based on dataset size and model requirements.\n",
    "- Larger batch sizes may improve training speed but require more memory.\n",
    "- Ensuring the test set remains separate is crucial for unbiased model evaluation.\n",
    "\n",
    "This data loading process forms the foundation for training and evaluating deep learning models using ECG data, ensuring structured and reproducible experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> useage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ecg_file(file, window_size=200, stride=100):\n",
    "    \"\"\"\n",
    "    Loads a single ECG .csv file and predicts label(s) using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file_path (str): Path to ECG .csv file.\n",
    "        checkpoint_path (str): Path to model checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted label for the file.\n",
    "    \"\"\"\n",
    "    # path_csv = r'mitbih_database\\{}.csv'.format(file_name)\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.replace(\"'\", \"\").str.strip()\n",
    "    feature_columns = df.columns[1:3]  # Assume MLII and V5\n",
    "    ecg_samples = df[feature_columns].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    ecg_samples = scaler.fit_transform(ecg_samples)\n",
    "    \n",
    "    # load devaice\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    latest_checkpoint = find_chackpoint()\n",
    "    #  Load checkpoint\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "\n",
    "    #  Initialize model with given parameters\n",
    "    model_args = {\"input_size\": 2, \"hidden_size\": 64, \"num_layers\": 2, \"output_size\": 5}\n",
    "    \n",
    "    model = LSTMModel(**model_args).to(device)\n",
    "\n",
    "    #  Load model and optimizer state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for start in range(0, len(ecg_samples) - window_size + 1, stride):\n",
    "        window = ecg_samples[start:start + window_size]\n",
    "        input_tensor = torch.tensor(window, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "            predictions.append(predicted_label)\n",
    "\n",
    "    if not predictions:\n",
    "        return None\n",
    "\n",
    "    prediction_df = pd.df = pd.DataFrame(predictions, columns=['predictions'])\n",
    "    count = prediction_df.value_counts()\n",
    "    \n",
    "    LBBB = counts_dict.get(1, 0)\n",
    "    RBBB = counts_dict.get(2, 0)\n",
    "    APB = counts_dict.get(3, 0)\n",
    "    PVC = (df['predictions'] == 4).mean() * 100\n",
    "    \n",
    "    return {\n",
    "        \"LBBB\": LBBB,\n",
    "        \"RBBB\": RBBB,\n",
    "        \"PACs\": APB,\n",
    "        \"PVCs\": PVC\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to improve model accurecy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
