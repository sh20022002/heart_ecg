{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  #  for progress bar\n",
    "import collections\n",
    "import random\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.csv_file_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith('.csv')]\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        #  Label mapping for classification\n",
    "        label_mapping = {'N': 0, 'L': 1, 'R': 2, 'A': 3, 'V': 4}\n",
    "\n",
    "        for file_path in self.csv_file_paths:\n",
    "            file_name = os.path.basename(file_path).replace('.csv', '')  # Get ECG file name (e.g., \"100\")\n",
    "            annotation_path = os.path.join(directory, f\"{file_name}annotations.txt\")  # Matching annotation file\n",
    "\n",
    "            #  Load ECG Data\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = df.columns.str.replace(\"'\", \"\").str.strip()  # Fix column names\n",
    "            feature_columns = df.columns[1:3]  # Select MLII & V5\n",
    "            ecg_samples = df[feature_columns].values\n",
    "\n",
    "            #  Load Annotations (Labels) with error handling\n",
    "            try:\n",
    "                with open(annotation_path, \"r\") as file:\n",
    "                    lines = file.readlines()[1:]  #  Skip header row\n",
    "\n",
    "                cleaned_data = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()  #  Split by whitespace\n",
    "                    if len(parts) >= 3:  #  Ensure at least 3 columns exist\n",
    "                        sample_idx, label = parts[1], parts[2]\n",
    "                        label = \"\".join(filter(str.isalpha, label))  #  Extract only valid label letters\n",
    "                        cleaned_data.append([int(sample_idx), label])\n",
    "\n",
    "                #  Convert to DataFrame\n",
    "                annotations = pd.DataFrame(cleaned_data, columns=[\"Sample\", \"Type\"])\n",
    "\n",
    "                #  Filter valid labels\n",
    "                annotations = annotations[annotations[\"Type\"].isin(label_mapping.keys())]\n",
    "                annotations[\"Label\"] = annotations[\"Type\"].map(label_mapping)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"🚨 Skipping {annotation_path} due to error: {e}\")\n",
    "                continue  #  Skip this file instead of crashing\n",
    "            \n",
    "            #  Match Labels to ECG Samples\n",
    "            labels = np.zeros(len(df), dtype=np.int64)  # Default all labels to 'N' (0)\n",
    "            for _, row in annotations.iterrows():\n",
    "                sample_idx = int(row['Sample'])  # Ensure it's an integer index\n",
    "                if 0 <= sample_idx < len(labels):  # Ensure index is valid\n",
    "                    labels[sample_idx] = row['Label']  # Assign label\n",
    "\n",
    "            self.data.append(ecg_samples)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        self.labels = np.concatenate(self.labels, axis=0)\n",
    "\n",
    "        #  Print label distribution before balancing\n",
    "        label_counts = collections.Counter(self.labels.tolist())\n",
    "        print(\"📌 Before Balancing - Label Counts:\", label_counts)\n",
    "\n",
    "        #  Balance dataset by undersampling class '0'\n",
    "        zero_class_indices = [i for i, label in enumerate(self.labels) if label == 0]\n",
    "        non_zero_indices = [i for i, label in enumerate(self.labels) if label != 0]\n",
    "\n",
    "        #  Keep a maximum of 2x the non-zero labels for class '0'\n",
    "        random.shuffle(zero_class_indices)\n",
    "        zero_class_sample_size = min(len(zero_class_indices), len(non_zero_indices) * 2)\n",
    "        zero_class_indices = zero_class_indices[:zero_class_sample_size]\n",
    "\n",
    "        #  Combine balanced dataset\n",
    "        selected_indices = zero_class_indices + non_zero_indices\n",
    "        random.shuffle(selected_indices)\n",
    "\n",
    "        #  Update dataset\n",
    "        self.data = self.data[selected_indices]\n",
    "        self.labels = self.labels[selected_indices]\n",
    "\n",
    "        #  Print label distribution after balancing\n",
    "        label_counts = Counter(self.labels.tolist())\n",
    "        print(\"📌 After Balancing - Label Counts:\", label_counts)\n",
    "\n",
    "        #  **Normalize Data**\n",
    "        print(\"📌 Applying StandardScaler normalization...\")\n",
    "        scaler = StandardScaler()\n",
    "        self.data = scaler.fit_transform(self.data)  # Normalize features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        sample = sample.unsqueeze(0)  # ✅ Ensure shape (seq_length, input_size)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.3, output_size=5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #  Add dropout layer to prevent overfitting\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_size, self.num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        #  Add batch normalization for stable training\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Ensure input is 3D (batch_size, seq_length, input_size)\n",
    "        if x.dim() == 2:  \n",
    "            x = x.unsqueeze(1)  # Add sequence_length = 1 if missing\n",
    "\n",
    "        #  Ensure hidden state batch size matches input batch size\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        #  Forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Pass hidden states\n",
    "        out = self.batch_norm(out[:, -1, :])  # Apply batch normalization\n",
    "        out = self.fc(out)  # Fully connected layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the LSTM Model for ECG Data\n",
    "\n",
    "The `LSTMModel` class defines a Long Short-Term Memory (LSTM) neural network designed for processing ECG data. LSTMs are particularly effective for sequential data, making them suitable for analyzing heartbeat patterns over time.\n",
    "\n",
    "### Explanation of the Components\n",
    "\n",
    "- **`input_size`**: Defines the number of input features per time step (e.g., number of ECG leads used).\n",
    "- **`hidden_size`**: Specifies the number of hidden units in each LSTM layer, controlling model complexity.\n",
    "- **`num_layers`**: Determines how many stacked LSTM layers are used for learning complex patterns.\n",
    "- **`output_size`**: Sets the dimension of the final output (e.g., number of classification categories).\n",
    "- **`lstm`**: The core sequential processing unit, which learns dependencies over time.\n",
    "- **`fc`**: A fully connected layer that maps the LSTM's last output to the desired prediction.\n",
    "\n",
    "### Explanation of the Forward Pass\n",
    "\n",
    "- Initializes hidden and cell states (`h0`, `c0`) as zero tensors, ensuring proper state tracking.\n",
    "- Feeds the input sequence `x` through the LSTM layers.\n",
    "- Extracts only the last time step’s output (`out[:, -1, :]`), as it holds the learned representation for classification.\n",
    "- Applies the fully connected layer (`fc`) to generate final predictions.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- LSTMs are well-suited for time-series and sequential analysis, making them effective for ECG classification.\n",
    "- The use of multiple LSTM layers helps capture long-term dependencies in heartbeat sequences.\n",
    "- Selecting only the last time step’s output ensures efficient prediction while reducing computational overhead.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Adjusting `hidden_size` and `num_layers` can significantly impact model performance.\n",
    "- Ensure input sequences have the correct shape (`batch_size, time_steps, input_size`).\n",
    "- Using GPU acceleration (`.to(device)`) improves training speed and efficiency.\n",
    "\n",
    "This LSTM model serves as a powerful tool for ECG analysis, capable of learning complex sequential dependencies and making accurate predictions based on time-series heartbeat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, return_logits=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataloader.\n",
    "\n",
    "    Works for both mid-training and post-training evaluation.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for evaluation.\n",
    "        device (torch.device): The device to run evaluation on.\n",
    "        return_logits (bool): If True, returns logits along with loss and accuracy.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_loss, accuracy) or (avg_loss, accuracy, logits)\n",
    "    \"\"\"\n",
    "    was_training = model.training  # Store if model was in training mode\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_logits = [] if return_logits else None  # Collect logits if needed\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for validation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "            \n",
    "            outputs = model(inputs)  #  Fixed variable name (`inputs` instead of `sampels`)\n",
    "            loss = F.cross_entropy(outputs, targets)  #  Compute loss correctly\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)  # Get predictions\n",
    "            correct += (predicted == targets).sum().item()  #  Correct accuracy calculation\n",
    "            total += targets.size(0)\n",
    "\n",
    "            if return_logits:\n",
    "                all_logits.append(outputs.cpu())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader))  #  Avoid division by zero\n",
    "    accuracy = correct / max(1, total)  #  Avoid division by zero\n",
    "\n",
    "    model.train(was_training)  # Restore original training state\n",
    "\n",
    "    if return_logits:\n",
    "        return avg_loss, accuracy, torch.cat(all_logits, dim=0)  #  Return logits if needed\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Model Evaluation Process\n",
    "\n",
    "Evaluating the model is essential for assessing its performance during and after training. The `evaluate` function helps compute loss and accuracy on a validation or test dataset to monitor model progress.\n",
    "\n",
    "### How the Evaluation Function Works\n",
    "\n",
    "This function takes in a trained model, a dataset loader, and a device specification (`CPU` or `GPU`) to evaluate model performance.\n",
    "\n",
    "### key Considerations\n",
    "\n",
    "- Ensure the evaluation dataset is separate from the training data.\n",
    "- Use a consistent batch size during evaluation for stable results.\n",
    "- If using a multi-class classification problem, adapt `F.cross_entropy` accordingly.\n",
    "\n",
    "This evaluation function provides an efficient way to assess model performance, ensuring continuous monitoring and improvement throughout training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(epoch, model, batch_idx, optimizer, loss, accuracy, path=\"model/best_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint only if:\n",
    "    1. No previous model exists.\n",
    "    2. New model has a higher accuracy than the previous one.\n",
    "    \n",
    "    Deletes inferior models automatically.\n",
    "    \"\"\"\n",
    "    checkpoint_dir = os.path.dirname(path)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if checkpoint_dir and not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        print(f\"📁 Created directory: {checkpoint_dir}\")\n",
    "\n",
    "    # Check if a previous checkpoint exists\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            prev_checkpoint = torch.load(path, map_location=\"cpu\")  # Load safely\n",
    "            prev_accuracy = prev_checkpoint.get('accuracy', 0)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading existing checkpoint: {e}\")\n",
    "            prev_accuracy = 0  # Assume no valid previous accuracy\n",
    "\n",
    "        # Keep only if accuracy is better\n",
    "        if accuracy <= prev_accuracy:\n",
    "            print(f\"🚫 New model NOT saved. Accuracy {accuracy:.4f} ≤ {prev_accuracy:.4f}\")\n",
    "            return  # Skip saving\n",
    "\n",
    "        # Delete previous checkpoint (ensuring it's removed)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "            print(f\"🗑️ Deleted inferior checkpoint (Accuracy: {prev_accuracy:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to delete old checkpoint: {e}\")\n",
    "\n",
    "    #  Save the best model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'batch_idx': batch_idx,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }, path)\n",
    "\n",
    "    print(f\" New Best Checkpoint Saved at {path} (Accuracy: {accuracy:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Checkpointing\n",
    "\n",
    "Saving and loading model checkpoints is crucial for maintaining training progress, preventing data loss, and resuming interrupted training efficiently. The `save_checkpoint` and `load_checkpoint` functions manage this process by storing and retrieving model parameters.\n",
    "\n",
    "### Saving Model Checkpoints\n",
    "\n",
    "This function saves the model state along with training metadata, such as the current epoch, batch index, optimizer state, and performance metrics.\n",
    "\n",
    "\n",
    "\n",
    "### Loading Model Checkpoints\n",
    "\n",
    "This function restores a saved model checkpoint, enabling training continuation without losing progress.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Prevents loss of progress due to system crashes or interruptions.\n",
    "- Allows for training continuation across different sessions or machines.\n",
    "- Enables model fine-tuning by starting from a pre-trained checkpoint.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Ensure the correct model and optimizer classes are passed when loading.\n",
    "- Store checkpoints periodically to avoid losing significant progress.\n",
    "- If resuming on a different machine, verify that the saved checkpoint is compatible with the new environment.\n",
    "\n",
    "By implementing checkpointing, deep learning workflows become more efficient, minimizing training redundancy and ensuring reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(model_class, optimizer_class, device, checkpoint_dir=\"model/\", model_args=None, optimizer_args=None):\n",
    "    \"\"\"\n",
    "    Loads the latest model and optimizer checkpoint from the given directory.\n",
    "\n",
    "    Parameters:\n",
    "        model_class (torch.nn.Module): Model class to initialize the model.\n",
    "        optimizer_class (torch.optim.Optimizer): Optimizer class to initialize the optimizer.\n",
    "        device (torch.device): Device to load the model on.\n",
    "        checkpoint_dir (str): Directory where checkpoints are stored.\n",
    "        model_args (dict, optional): Dictionary of arguments for model initialization.\n",
    "        optimizer_args (dict, optional): Dictionary of arguments for optimizer initialization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, optimizer, start_epoch, start_batch_idx, loss, accuracy)\n",
    "    \"\"\"\n",
    "    #  Find all checkpoint files matching \"checkpoint_epoch_*.pth\"\n",
    "    checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pth\")))\n",
    "\n",
    "    if not checkpoint_files:\n",
    "        print(f\"⚠️ No checkpoint found in '{checkpoint_dir}'. Starting from scratch.\")\n",
    "        return None, None, 0, 0, None, None  # Start from epoch 0\n",
    "\n",
    "    #  Select the latest checkpoint (highest epoch number)\n",
    "    latest_checkpoint = checkpoint_files[-1]  # Last file in sorted list\n",
    "\n",
    "    #  Load checkpoint\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "\n",
    "    #  Initialize model with given parameters\n",
    "    model_args = model_args or {}\n",
    "    model = model_class(**model_args).to(device)\n",
    "\n",
    "    #  Initialize optimizer with given parameters\n",
    "    optimizer_args = optimizer_args or {\"lr\": 0.001}\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_args)\n",
    "\n",
    "    #  Load model and optimizer state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint.get('epoch', 0)\n",
    "    start_batch_idx = checkpoint.get('batch_idx', 0)\n",
    "    loss = checkpoint.get('loss', None)\n",
    "    accuracy = checkpoint.get('accuracy', None)\n",
    "\n",
    "    print(f\" Loaded checkpoint from {latest_checkpoint} (Epoch {start_epoch}, Batch {start_batch_idx}).\")\n",
    "\n",
    "    return model, optimizer, start_epoch, start_batch_idx, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, criterion, device, total_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the model, resuming from the last checkpoint if available.\n",
    "    \n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): Training data loader.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Training device (CPU/GPU).\n",
    "        total_epochs (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    model_args = {\"input_size\": 2, \"hidden_size\": 64, \"num_layers\": 2, \"output_size\": 5}\n",
    "    optimizer_args = {\"lr\": 0.001}\n",
    "\n",
    "    #  Load latest checkpoint or start fresh\n",
    "    model, optimizer, start_epoch, start_batch_idx, loss, accuracy = load_checkpoint(\n",
    "        model_class=LSTMModel, \n",
    "        optimizer_class=torch.optim.Adam, \n",
    "        device=device,\n",
    "        checkpoint_dir=\"model/\",\n",
    "        model_args=model_args, \n",
    "        optimizer_args=optimizer_args\n",
    "    )\n",
    "\n",
    "    #  Handle missing checkpoint (Initialize a new model if needed)\n",
    "    if model is None:\n",
    "        model = LSTMModel(**model_args).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        start_epoch, start_batch_idx = 0, 0\n",
    "        print(\"⚠️ No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "    model.train()  #  Ensure model is in training mode\n",
    "    criterion.to(device)  # Move loss function to the correct device\n",
    "\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        print(f\"\\n🚀 Started Epoch {epoch+1}/{total_epochs}\")\n",
    "        for batch_idx, (samples, labels) in enumerate(train_loader):\n",
    "            #  Skip already processed batches in the current epoch\n",
    "            if epoch == start_epoch and batch_idx < start_batch_idx:\n",
    "                continue  \n",
    "\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(samples)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # eval\n",
    "        avg_loss, accuracy = evaluate(model, train_loader, device, return_logits=False)\n",
    "        \n",
    "        #  Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(epoch, model, batch_idx, optimizer, loss, accuracy, path=f\"model/checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{total_epochs}], Batch {batch_idx}/{len(train_loader)} - Val Loss = {avg_loss:.4f}, Val Accuracy = {accuracy:.4%}\")\n",
    "\n",
    "    print(\"\\n🎉 Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Model Training Process\n",
    "\n",
    "Training a deep learning model involves iterating through the dataset, optimizing weights, and monitoring performance over multiple epochs. The `train` function facilitates this process while incorporating checkpointing for training resumption.\n",
    "\n",
    "### How the Training Function Works\n",
    "\n",
    "This function takes a dataset loader, a loss function, and a device specification to train the model. It also supports saving and resuming training from a checkpoint.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Allows efficient training with automatic resumption.\n",
    "- Provides periodic evaluation to track learning progress.\n",
    "- Enables early stopping or hyperparameter tuning based on validation results.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- Ensure batch skipping logic works correctly when resuming training.\n",
    "- Adjust learning rates dynamically if performance stagnates.\n",
    "- Regularly monitor validation accuracy to detect overfitting.\n",
    "\n",
    "This training function ensures a robust workflow, combining efficient model training with checkpointing and evaluation to optimize ECG data analysis models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming paths and data are correct\n",
    "data_path = 'mitbih_database'\n",
    "dataset = ECGDataset(data_path)\n",
    "\n",
    "print(f'Dataset size: {dataset.__len__()}')\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Runing on : cuda') if torch.cuda.is_available() else print('Runing on : cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "train(train_loader, criterion, device, total_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data Loading for ECG Training\n",
    "\n",
    "Properly loading and splitting a dataset is crucial for effective model training and evaluation. The following approach ensures that the ECG dataset is structured for training while maintaining a separate test set for validation.\n",
    "\n",
    "### How Data is Loaded and Split\n",
    "\n",
    "- Loads the ECG dataset from the specified directory.\n",
    "- Ensures all `.csv` files in the directory are processed correctly.\n",
    "\n",
    "* Splits the dataset into an **80% training set** and a **20% test set**.\n",
    "* Uses `torch.utils.data.random_split()` to randomly partition the data.\n",
    "\n",
    "- Creates `DataLoader` objects for both the training and test datasets.\n",
    "- Enables **batch processing** with a batch size of 32.\n",
    "- `shuffle=True` is set for the training data to prevent model overfitting to specific data patterns.\n",
    "- The test dataset is not shuffled to maintain consistency in evaluation.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- Ensures a proper training-test split for model evaluation.\n",
    "- Facilitates efficient data loading using PyTorch's `DataLoader`.\n",
    "- Randomization in the training set prevents the model from memorizing specific sequences.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- The dataset split ratio can be adjusted based on dataset size and model requirements.\n",
    "- Larger batch sizes may improve training speed but require more memory.\n",
    "- Ensuring the test set remains separate is crucial for unbiased model evaluation.\n",
    "\n",
    "This data loading process forms the foundation for training and evaluating deep learning models using ECG data, ensuring structured and reproducible experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Fine-Tuning Options for Model Optimization\n",
    "\n",
    "| **Issue**                           | **Fix**                                      |\n",
    "|-------------------------------------|----------------------------------------------|\n",
    "| Accuracy **stuck below 65%**       | Increase `hidden_size = 128`, reduce `lr = 0.001` |\n",
    "| Accuracy **fluctuates too much**   | Increase batch size: `batch_size=64`         |\n",
    "| Model overfitting (accuracy high, but loss stops decreasing) | Add dropout: `nn.Dropout(0.3)` |\n",
    "\n",
    "🔹 **Tip:** Experiment with one change at a time and observe the impact before making multiple changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> useage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
